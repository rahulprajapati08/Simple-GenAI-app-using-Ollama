# ğŸ¤– Simple GenAI App using Ollama (Gemma3:1B) + Streamlit

This is a minimal yet powerful **Generative AI App** built using **Streamlit** and the **Gemma3:1B model** via **Ollama**. It allows users to interact with LLM â€” just type your question and get human-like responses in real time.

---

## ğŸ§  What It Does

- ğŸ—¨ï¸ Ask any question, get a meaningful answer like a standard LLM
- âš¡ Runs entirely **locally** using [Ollama](https://ollama.com/)
- ğŸŒ Clean and intuitive **Streamlit UI**

---

## ğŸ“¸ App Screenshot

![image](https://github.com/user-attachments/assets/c6d853da-d021-4c12-b8df-c5a45a370a4b)


---

## ğŸš€ Technologies Used

- **Language Model**: [`gemma3:1b`](https://ollama.com/library/gemma) via Ollama
- **Frontend**: [Streamlit](https://streamlit.io/)
- **Backend Logic**: Python (`app.py`)
- **Model Serving**: Ollama (runs locally)

---

## âš™ï¸ How to Run the App Locally
1. **Install Ollama and pull the model**:
   ```bash
   ollama run gemma3:1b
2. Clone this repository:
    ```bash
   git clone https://github.com/rahulprajapati08/simple-genai-ollama.git
   cd simple-genai-ollama
3. Run the app:
   ```bash
   streamlit run app.py

---

## This app highlights:

 - My ability to work with LLMs locally and efficiently

 - Experience integrating modern model deployment tools like Ollama

 - Frontend-backend synergy using Streamlit

